# Production Bedrock AI Process

**AI-Powered Data Extraction using AWS Bedrock Foundation Models**

> **Service:** BedrockService  
> **Purpose:** Extract structured data from OCR text using LLMs  
> **Last Updated:** January 2025

---

## Table of Contents

1. [Overview](#overview)
2. [Supported Models](#supported-models)
3. [Model Selection](#model-selection)
4. [Processing Flow](#processing-flow)
5. [Prompt Architecture](#prompt-architecture)
6. [Configuration](#configuration)
7. [Caching Strategy](#caching-strategy)
8. [Token Management](#token-management)
9. [Troubleshooting](#troubleshooting)

---

## Overview

### What Bedrock Does

AWS Bedrock provides access to foundation models (LLMs) that:
- **Understand context** - Comprehend document structure and meaning
- **Extract data** - Identify and extract specific information
- **Structure output** - Format results as JSON
- **Handle ambiguity** - Resolve unclear or inconsistent data

### Service Architecture

```
???????????????????????????????????????????????????????????????????
?BEDROCK SERVICE ARCHITECTURE   ?
???????????????????????????????????????????????????????????????????
?  ?
?  [Textract OCR]   ?
?       ?  ?
?  [PromptService] ? Build Complete Prompt?
?   System Prompt + User Prompt + Rules + Examples  ?
?     ?    ?
?  [Check Prompt Cache]?
?       ? Cache Miss   ?
?  [Bedrock Model] ? Nova Lite / Qwen / Claude?
?       ? Response    ?
?  [Parse Response] ? Extract JSON  ?
?       ?   ?
?  [Validate JSON] ?
?    ?    ?
?  [Cache Response] ? For future identical prompts ?
?       ?    ?
?  [Return Structured Data]    ?
???????????????????????????????????????????????????????????????????
```

---

## Supported Models

### Currently Configured Models

| Model | Model ID | Region | Best For |
|-------|----------|--------|----------|
| **Nova Lite** | `amazon.nova-lite-v1:0` | us-west-1 | Fast, cost-effective |
| **Qwen 3** | `qwen.qwen2.5-7b-instruct` | us-west-2 | Balanced performance |
| **Claude 3 Haiku** | `anthropic.claude-3-haiku` | us-east-1 | High accuracy |
| **Titan Text Express** | `amazon.titan-text-express-v1` | us-east-1 | Basic extraction |

### Model Comparison

| Feature | Nova Lite | Qwen 3 | Claude 3 | Titan |
|---------|-----------|--------|----------|-------|
| **Speed** | ??? Fast | ?? Medium | ? Slower | ??? Fast |
| **Accuracy** | ??? Good | ???? Better | ????? Best | ?? Basic |
| **Cost** | ?? Low | ???? Medium | ?????? High | ?? Low |
| **Context Window** | 300K tokens | 32K tokens | 200K tokens | 8K tokens |
| **JSON Output** | ? Reliable | ? Reliable | ? Excellent | ?? Basic |

### Cost Per Request (Approximate)

```
Model     Input (per 1K tokens)   Output (per 1K tokens)
Nova Lite       $0.0004      $0.0012
Qwen 3       $0.0008         $0.0024
Claude 3 Haiku  $0.00025    $0.00125
Titan Express   $0.0008 $0.0016
```

**Typical Document (5,000 input + 1,000 output tokens):**
- Nova Lite: $0.0032
- Qwen 3: $0.0064
- Claude 3: $0.00250
- Titan: $0.0056

---

## Model Selection

### How to Choose a Model

```csharp
// In Function.cs

// Option 1: Nova Lite (Fast & Cheap)
var modelConfig = BedrockModelConfig.NovaLite;

// Option 2: Qwen 3 (Balanced) - CURRENT DEFAULT
var modelConfig = BedrockModelConfig.Qwen3;

// Option 3: Claude 3 (High Accuracy)
var modelConfig = BedrockModelConfig.Claude3Haiku;

// Option 4: Titan (Basic)
var modelConfig = BedrockModelConfig.TitanTextExpress;
```

### Model Selection Criteria

**Use Nova Lite when:**
? Processing high volumes  
? Cost is primary concern  
? Simple, structured documents  
? Fast turnaround needed

**Use Qwen 3 when:**
? Balanced cost/performance needed  
? Complex document structures  
? Good JSON formatting required  
? **DEFAULT CHOICE for production**

**Use Claude 3 when:**
? Highest accuracy required  
? Complex reasoning needed  
? Legal/critical documents  
? Budget allows premium cost

**Use Titan when:**
? Basic extraction only  
? AWS-native solution preferred  
? Simple key-value extraction

---

## Processing Flow

### End-to-End Flow

```
????????????????????????????????????????????????????????????????
? STEP 1: PREPARE INPUT  ?
????????????????????????????????????????????????????????????????
? Input: TextractResponse (OCR results)?
? ?
? Combine multiple documents:    ?
?   - RawText from all pages ?
?   - FormFields (merged)?
?   - TableData (combined)   ?
? ?
? Output: Combined text source?
????????????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????????
? STEP 2: BUILD PROMPT   ?
????????????????????????????????????????????????????????????????
? PromptService builds complete prompt:  ?
?      ?
? System Prompt: ?
?   - Extraction instructions ?
?   - JSON schema (V1 only) ?
?   - Rules (percentage, names, dates)   ?
?   - Examples (few-shot learning)?
?    ?
? User Prompt:   ?
?- Source data (OCR text)?
?- Specific extraction request   ?
????????????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????????
? STEP 3: CHECK PROMPT CACHE?
????????????????????????????????????????????????????????????????
? Cache Key = Hash(sourceData + systemPrompt + userPrompt)?
?    ?
? Cache HIT:?
?   ? Return cached response instantly  ?
?     ?
? Cache MISS:?
? ? Proceed to Step 4?
????????????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????????
? STEP 4: INVOKE BEDROCK MODEL ?
????????????????????????????????????????????????????????????????
? Create model-specific request:  ?
?  ?
? Nova/Qwen/Claude:  ?
?   { ?
?     "messages": [    ?
?       { "role": "system", "content": systemPrompt },  ?
?  { "role": "user", "content": userPrompt }?
?     ],?
?     "max_tokens": 4096,   ?
?     "temperature": 0.1?
?   }     ?
?  ?
? Send to Bedrock: ?
?   ModelId: qwen.qwen2.5-7b-instruct?
?   Region: us-west-2 ?
?    ?
? Wait for response (10-60 seconds) ?
????????????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????????
? STEP 5: PARSE RESPONSE ?
????????????????????????????????????????????????????????????????
? Response format varies by model:?
?  ?
? Qwen: response.choices[0].message.content   ?
? Nova: response.output.message.content[0].text   ?
? Claude: response.content  ?
? Titan: response.results[0].outputText   ?
?   ?
? Extract:  ?
?   - Output text (JSON string) ?
?   - Input tokens (for cost) ?
?   - Output tokens (for cost)  ?
????????????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????????
? STEP 6: EXTRACT AND VALIDATE JSON    ?
????????????????????????????????????????????????????????????????
? Clean response:  ?
?   1. Strip code fences (```json ... ```)?
?   2. Extract first JSON object/array   ?
?   3. Normalize structure   ?
?     ?
? Validate JSON:?
?   - Parse with JsonDocument?
?   - Check for malformed JSON   ?
?   - Handle duplicate keys  ?
?     ?
? Save artifacts:  ?
?   - completion_text_{timestamp}.txt?
?   - completion_json_{timestamp}.json?
?   - raw_response_{timestamp}.txt  ?
????????????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????????
? STEP 7: CACHE RESPONSE ?
????????????????????????????????????????????????????????????????
? Cache in memory (MemoryCache):?
?   Key: Prompt hash  ?
?   Value: { response, inputTokens, outputTokens }?
?   TTL: 60 minutes   ?
?   ?
? Future identical requests return instantly ?
????????????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????????
? STEP 8: RETURN STRUCTURED DATA ?
????????????????????????????????????????????????????????????????
? Return tuple:?
?   (string response, int inputTokens, int outputTokens)  ?
?   ?
? Caller uses response for: ?
?   - V1: Schema-based extraction ?
?   - V2: Dynamic extraction?
????????????????????????????????????????????????????????????????
```

---

## Prompt Architecture

### System Prompt Structure

```
???????????????????????????????????????????????
?  SYSTEM PROMPT COMPONENTS?
???????????????????????????????????????????????
? 1. Role & Context?
?    "You are an expert document analyzer..." ?
?  ?
? 2. Task Description  ?
?    "Extract structured data from OCR text..." ?
?    ?
? 3. JSON Schema (V1 only)  ?
?    Schema defines expected output structure  ?
?  ?
? 4. Extraction Rules?
?    - Percentage calculation rules ?
? - Name parsing rules?
?    - Date format rules  ?
?    ?
? 5. Examples (Few-Shot)   ?
?    Input ? Output examples?
?   ?
? 6. Output Instructions ?
?  "Return only valid JSON. No explanations."  ?
???????????????????????????????????????????????
```

### User Prompt Structure

```
???????????????????????????????????????????????
?  USER PROMPT COMPONENTS   ?
???????????????????????????????????????????????
? 1. Instruction ?
?    "Extract and analyze ALL data below..."   ?
?   ?
? 2. Source Data ?
?    === Document 1 === ?
?    RAW TEXT:  ?
?    {textract raw text}?
?  ?
?    FORM FIELDS: ?
?    {key: value pairs}   ?
? ?
?    TABLE DATA:?
?    {table contents}?
? ?
?    === Document 2 ===  ?
?    ... ?
?    ?
? 3. Specific Requirements (V2 only) ?
?    "Focus on: Buyer/Seller info, Property..." ?
???????????????????????????????????????????????
```

### V1 vs V2 Prompts

| Aspect | V1 (Schema-Based) | V2 (Dynamic) |
|--------|-------------------|--------------|
| **Schema** | ? Provided | ? None |
| **Rules** | ? Specific rules | ? AI-inferred |
| **Examples** | ? Few-shot | ? Zero-shot |
| **Output** | ? Constrained | ? Flexible |
| **Use Case** | Consistent extraction | Discovery |

---

## Configuration

### Model Configuration

```csharp
// BedrockModelConfig.cs

public static BedrockModelConfig Qwen3 = new()
{
    ModelId = "qwen.qwen2.5-7b-instruct",
    SystemPrompt = "You are an expert document analyzer...",
  RequestFormat = RequestFormat.Qwen,
    ResponseFormat = ResponseFormat.Qwen,
    InferenceParameters = new BedrockInferenceParameters
    {
        MaxTokens = 4096,
    Temperature = 0.1f,
     TopP = 0.95f,
        StopSequences = new[] { "</response>" }
    }
};
```

### Inference Parameters

| Parameter | Purpose | Recommended Value |
|-----------|---------|-------------------|
| **MaxTokens** | Maximum response length | 4096 (structured data) |
| **Temperature** | Response randomness | 0.1 (deterministic) |
| **TopP** | Nucleus sampling | 0.95 (focused) |
| **StopSequences** | Early termination | `["</response>"]` |

### Temperature Guide

```
0.0 = Fully deterministic (identical outputs)
0.1 = Mostly deterministic (RECOMMENDED for extraction)
0.5 = Balanced creativity
0.9 = Highly creative (NOT for extraction)
```

### Region Configuration

```csharp
// Automatic region routing based on model
private IAmazonBedrockRuntime CreateBedrockClientForModel(BedrockModelConfig modelConfig)
{
    var region = modelConfig.ModelId switch
    {
      var id when id.Contains("nova-lite") => Amazon.RegionEndpoint.USWest1,
        var id when id.Contains("nova") => Amazon.RegionEndpoint.USWest1,
 var id when id.Contains("qwen") => Amazon.RegionEndpoint.USWest2,
        _ => Amazon.RegionEndpoint.USEast1
    };
    
    return new AmazonBedrockRuntimeClient(region);
}
```

---

## Caching Strategy

### Why Cache Bedrock Responses?

? **Cost Savings** - Avoid duplicate model invocations ($0.003-$0.006 per request)  
? **Speed** - Cached responses return instantly (< 1ms vs 10-60 seconds)  
? **Consistency** - Same input always returns same output  
? **Development** - Test without consuming API quota

### Cache Implementation

```csharp
// Prompt-based caching
var cacheInput = $"{sourceData}|{systemPrompt}|{userPrompt}|{modelConfig.ModelId}";
var promptCacheKey = $"prompt_{CalculateHash(cacheInput)}";

// Check cache
if (_cache.TryGetValue<CachedResponse>(promptCacheKey, out var cached))
{
    return (cached.Response, cached.InputTokens, cached.OutputTokens);
}

// ... invoke model ...

// Cache response
_cache.Set(promptCacheKey, new CachedResponse
{
    Response = outputText,
    InputTokens = inputTokens,
    OutputTokens = outputTokens
}, TimeSpan.FromMinutes(60));
```

### Cache Key Calculation

```csharp
private static string CalculateHash(string input)
{
    using var sha256 = SHA256.Create();
    var bytes = Encoding.UTF8.GetBytes(input);
    var hash = sha256.ComputeHash(bytes);
return Convert.ToBase64String(hash);
}
```

**Cache key includes:**
- Source data (OCR text)
- System prompt (instructions + schema)
- User prompt (specific request)
- Model ID (different models = different cache)

### Cache Behavior

```
First Request:
  Prompt ? Bedrock Model ? Cache ? Return
  Time: 10-60 seconds
  Cost: $0.003-$0.006

Identical Request:
  Prompt ? Cache Hit ? Return
  Time: < 1 ms
  Cost: $0.00
```

### Cache Duration

- **Memory Cache (MemoryCache):** 60 minutes
- **Cleared on:** Application restart, memory pressure
- **Storage:** In-memory (not persisted to disk)

---

## Token Management

### Token Counting

Bedrock returns actual token counts:

```csharp
// Response includes
{
    "inputTokens": 4567,    // Prompt tokens
    "outputTokens": 892     // Response tokens
}
```

### Token Estimation

When actual counts unavailable:

```csharp
private static int EstimateTokenCount(string text)
{
    return text?.Length / 4 ?? 0;
}
```

### Cost Calculation

```csharp
private decimal CalculateCost(int inputTokens, int outputTokens)
{
    // Qwen 3 pricing
    const decimal INPUT_COST_PER_1K_TOKENS = 0.0008m;
    const decimal OUTPUT_COST_PER_1K_TOKENS = 0.0024m;
    
    var inputCost = (inputTokens / 1000m) * INPUT_COST_PER_1K_TOKENS;
    var outputCost = (outputTokens / 1000m) * OUTPUT_COST_PER_1K_TOKENS;
    
    return inputCost + outputCost;
}
```

### Token Limits

| Model | Input Limit | Output Limit | Total Context |
|-------|-------------|--------------|---------------|
| Nova Lite | 300K | 4K | 300K |
| Qwen 3 | 32K | 4K | 32K |
| Claude 3 | 200K | 4K | 200K |
| Titan | 8K | 4K | 8K |

### Managing Token Usage

**If hitting token limits:**

1. **Reduce input:**
 ```csharp
   // Truncate long documents
   if (combinedData.Length > 30000)
   {
       combinedData = combinedData.Substring(0, 30000);
   }
   ```

2. **Summarize first:**
   ```csharp
   // Extract key sections only
   var summary = ExtractKeyInformation(textractResponse);
```

3. **Process in chunks:**
   ```csharp
   // Split large documents
   var chunks = SplitIntoChunks(textractResponse, 5000);
   foreach (var chunk in chunks)
   {
       await ProcessChunk(chunk);
   }
   ```

---

## Troubleshooting

### Issue: "Model returned malformed JSON"

**Symptoms:**
```
Could not extract valid JSON from response
```

**Causes:**
- Model included explanation text
- JSON wrapped in code fences
- Incomplete JSON response

**Solutions:**

1. **Check raw response:**
   ```
   CachedFiles_OutputFiles/raw_response_{timestamp}.txt
   ```

2. **Improve prompt:**
   ```
   "Return ONLY valid JSON. No explanations, no markdown, no code fences."
   ```

3. **Use JSON mode** (if model supports it)

---

### Issue: Slow Response Times

**Symptoms:**
- Bedrock takes > 60 seconds
- Timeouts occur

**Solutions:**

1. **Reduce input size:**
 ```csharp
   var summary = ExtractKeyInformation(textractResponse);
   ```

2. **Lower max_tokens:**
   ```csharp
   MaxTokens = 2048 // Instead of 4096
   ```

3. **Try faster model:**
   ```csharp
   var modelConfig = BedrockModelConfig.NovaLite; // Fastest
   ```

---

### Issue: High Costs

**Symptoms:**
- AWS bill higher than expected
- Many Bedrock invocations

**Solutions:**

1. **Enable caching:**
   ```csharp
   // Already enabled by default
   ```

2. **Monitor token usage:**
   ```csharp
   Console.WriteLine($"Tokens: {inputTokens} in, {outputTokens} out");
   Console.WriteLine($"Cost: ${cost:F4}");
 ```

3. **Use cheaper model:**
   ```csharp
   var modelConfig = BedrockModelConfig.NovaLite; // Cheapest
   ```

4. **Batch processing:**
   ```csharp
   // Process multiple documents in single prompt
   ```

---

### Issue: Incorrect Extractions

**Symptoms:**
- Missing fields
- Wrong values
- Inconsistent formatting

**Solutions:**

1. **Add more examples:**
   ```
   Prompts/Examples/default/example3.json
   ```

2. **Improve rules:**
   ```
   Prompts/Rules/custom_rule.md
   ```

3. **Try more accurate model:**
   ```csharp
   var modelConfig = BedrockModelConfig.Claude3Haiku; // Most accurate
   ```

4. **Validate OCR quality:**
   - Check Textract confidence scores
   - Review raw text extraction

---

## Next Steps

After Bedrock extraction:

1. **Schema mapping** ? [PRODUCTION_SCHEMA_MAPPER.md](PRODUCTION_SCHEMA_MAPPER.md)
2. **Review V1 vs V2** ? Compare outputs
3. **Analyze extensions** ? Schema improvements

---

## Quick Reference

### Switch Models
```csharp
// In Function.cs
var modelConfig = BedrockModelConfig.Qwen3;       // Balanced (DEFAULT)
var modelConfig = BedrockModelConfig.NovaLite;    // Fast & cheap
var modelConfig = BedrockModelConfig.Claude3Haiku; // Most accurate
```

### Invoke Bedrock
```csharp
(string response, int inputTokens, int outputTokens) = 
    await _bedrockService.ProcessTextractResults(
        sourceData,
        systemPrompt,
userPrompt,
        context
    );
```

### Calculate Cost
```csharp
decimal cost = (inputTokens / 1000m) * 0.0008m + 
  (outputTokens / 1000m) * 0.0024m;
```

---

**Ready for dual-version extraction?** See [PRODUCTION_SCHEMA_MAPPER.md](PRODUCTION_SCHEMA_MAPPER.md) for V1 + V2 processing.
